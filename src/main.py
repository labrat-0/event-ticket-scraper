"""
Apify Actor entry point for Event Ticket Scraper.

Handles actor lifecycle, free tier enforcement,
batch push with max_results guard, and state persistence.
"""

from __future__ import annotations

import logging
import os

import httpx
from apify import Actor

from .models import ScraperInput
from .scraper import EventTicketScraper

logger = logging.getLogger("src")

# Free tier: 25 results max for non-paying users
_FREE_TIER_LIMIT = 25
_BATCH_SIZE = 25


async def main() -> None:
    async with Actor:
        actor_input = await Actor.get_input() or {}
        config = ScraperInput.from_actor_input(actor_input)

        # Validate input
        validation_error = config.validate_input()
        if validation_error:
            logger.error("Input validation failed: %s", validation_error)
            await Actor.set_status_message(f"Error: {validation_error}")
            await Actor.fail(status_message=validation_error)
            return

        # Determine result limit (free tier enforcement)
        is_at_home = os.getenv("APIFY_IS_AT_HOME", "").lower() in ("1", "true")
        is_paying = os.getenv("APIFY_USER_IS_PAYING", "").lower() in ("1", "true")
        max_results = config.max_results
        if is_at_home and not is_paying:
            max_results = min(max_results, _FREE_TIER_LIMIT)
            logger.info("Free tier: limiting to %d results", max_results)

        # Status message
        mode_desc = {
            "search": f"Searching events for '{config.keyword}'",
            "get_event": f"Looking up event: {config.event_id}",
            "venues": f"Searching venues for '{config.keyword}'",
        }
        await Actor.set_status_message(
            f"{mode_desc.get(config.mode, 'Processing')} (max {max_results} results)..."
        )

        dataset = await Actor.open_dataset()

        # State persistence for resume
        state = await Actor.use_state(default_value={"total_pushed": 0})
        total_pushed: int = state.get("total_pushed", 0)
        batch: list[dict] = []

        async with httpx.AsyncClient(
            follow_redirects=True,
            limits=httpx.Limits(max_connections=10, max_keepalive_connections=5),
        ) as http_client:
            scraper = EventTicketScraper(config, http_client)

            async for record in scraper.run():
                if total_pushed >= max_results:
                    logger.info(
                        "Reached max results (%d), stopping", max_results
                    )
                    break

                batch.append(record)

                if len(batch) >= _BATCH_SIZE:
                    remaining = max_results - total_pushed
                    flush = batch[:remaining]
                    await dataset.push_data(flush)
                    total_pushed += len(flush)
                    state["total_pushed"] = total_pushed

                    item_type = "event(s)" if config.mode != "venues" else "venue(s)"
                    await Actor.set_status_message(
                        f"Found {total_pushed} {item_type}..."
                    )
                    logger.info(
                        "Pushed batch of %d (total: %d)",
                        len(flush),
                        total_pushed,
                    )
                    batch = []

                    if total_pushed >= max_results:
                        break

        # Flush remaining
        if batch and total_pushed < max_results:
            remaining = max_results - total_pushed
            flush = batch[:remaining]
            await dataset.push_data(flush)
            total_pushed += len(flush)
            state["total_pushed"] = total_pushed

        item_type = "event(s)" if config.mode != "venues" else "venue(s)"
        logger.info("Scraping complete. Total records: %d", total_pushed)
        await Actor.set_status_message(
            f"Done! Found {total_pushed} {item_type}."
        )
